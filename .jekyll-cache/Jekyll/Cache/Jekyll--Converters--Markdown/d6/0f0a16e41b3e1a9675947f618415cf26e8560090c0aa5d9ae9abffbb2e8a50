I"⁄<p>I am pleased to announce that our research paper titled ‚ÄúSupporting Requesters in Writing Clear Crowdsourcing Task Descriptions Through Computational Flaw Assessment‚Äù has been accepted for presentation at the main technical track of Intelligent User Interfaces 2023 (IUI23).</p>

<p>In this work, we developed a interactive system named ClarifyIT to enable microtask crowdsourcing requesters to iteratively identify and correct some of the common clarity flaws found in crowdsourcing task descriptions, that have shown to degrade the quality of data collected from workers. Unlike traditional approaches that involve the assistance of workers or experts in asssiting requesters to develop clear task descriptions that require hours, if not days, and significant financial contraint, we employed NLP models trained on real-world task descriptions that provided help in milliseconds without having to spend a single penny. :money_mouth_face:</p>

<object data="assets/img/ClarifyIT_new.pdf" type="application/pdf" width="700px" height="700px">
    <embed src="assets/img/ClarifyIT_new.pdf" />
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="assets/img/ClarifyIT_new.pdf">Download PDF</a>.</p>
    &lt;/embed&gt;
</object>

<p>In our evaluation, we not only assessed the usability of ClarifyIT with requesters but also tested the system with actual workers to ensure that the task descriptions produced using ClarifyIT were perceived as high quality. The results of our study indicated that <strong>65%</strong> of requesters found the tool to be helpful or very helpful, and <strong>76%</strong> of workers believed that the overall clarity of task descriptions improved when created using ClarifyIT.</p>
:ET